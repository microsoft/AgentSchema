// Code generated by AgentSchema emitter; DO NOT EDIT.

use agentschema::*;

#[test]
fn test_model_load_json() {
    let json_data = r####"
{
  "id": "gpt-35-turbo",
  "provider": "azure",
  "apiType": "chat",
  "connection": {
    "kind": "key",
    "endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
    "key": "{your-api-key}"
  },
  "options": {
    "type": "chat",
    "temperature": 0.7,
    "maxTokens": 1000
  }
}
"####;
    let instance = Model::from_json(json_data).expect("Failed to load Model from JSON");
    assert_eq!(instance.id, "gpt-35-turbo");
    assert_eq!(instance.provider.as_deref(), Some("azure"));
    assert_eq!(instance.api_type.as_deref(), Some("chat"));
}

#[test]
fn test_model_load_yaml() {
    let yaml_data = r####"
id: gpt-35-turbo
provider: azure
apiType: chat
connection:
  kind: key
  endpoint: "https://{your-custom-endpoint}.openai.azure.com/"
  key: "{your-api-key}"
options:
  type: chat
  temperature: 0.7
  maxTokens: 1000

"####;
    let instance = Model::from_yaml(yaml_data).expect("Failed to load Model from YAML");
    assert_eq!(instance.id, "gpt-35-turbo");
    assert_eq!(instance.provider.as_deref(), Some("azure"));
    assert_eq!(instance.api_type.as_deref(), Some("chat"));
}

#[test]
fn test_model_roundtrip() {
    let json_data = r####"
{
  "id": "gpt-35-turbo",
  "provider": "azure",
  "apiType": "chat",
  "connection": {
    "kind": "key",
    "endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
    "key": "{your-api-key}"
  },
  "options": {
    "type": "chat",
    "temperature": 0.7,
    "maxTokens": 1000
  }
}
"####;
    let instance = Model::from_json(json_data).expect("Failed to load Model");
    let json_out = instance
        .to_json()
        .expect("Failed to serialize Model to JSON");
    let reloaded = Model::from_json(&json_out).expect("Failed to reload Model from JSON");
    assert_eq!(reloaded.id, "gpt-35-turbo");
    assert_eq!(reloaded.provider.as_deref(), Some("azure"));
    assert_eq!(reloaded.api_type.as_deref(), Some("chat"));
}

#[test]
fn test_model_to_json() {
    let json_data = r####"
{
  "id": "gpt-35-turbo",
  "provider": "azure",
  "apiType": "chat",
  "connection": {
    "kind": "key",
    "endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
    "key": "{your-api-key}"
  },
  "options": {
    "type": "chat",
    "temperature": 0.7,
    "maxTokens": 1000
  }
}
"####;
    let instance = Model::from_json(json_data).expect("Failed to load Model");
    let json_out = instance.to_json().expect("Failed to convert Model to JSON");
    // Verify the output is valid JSON
    let _: serde_json::Value =
        serde_json::from_str(&json_out).expect("Generated JSON is not valid");
}

#[test]
fn test_model_to_yaml() {
    let json_data = r####"
{
  "id": "gpt-35-turbo",
  "provider": "azure",
  "apiType": "chat",
  "connection": {
    "kind": "key",
    "endpoint": "https://{your-custom-endpoint}.openai.azure.com/",
    "key": "{your-api-key}"
  },
  "options": {
    "type": "chat",
    "temperature": 0.7,
    "maxTokens": 1000
  }
}
"####;
    let instance = Model::from_json(json_data).expect("Failed to load Model");
    let yaml_out = instance.to_yaml().expect("Failed to convert Model to YAML");
    // Verify the output is valid YAML by re-loading
    let _: serde_json::Value =
        serde_yaml::from_str(&yaml_out).expect("Generated YAML is not valid");
}

#[test]
fn test_model_from_model() {
    // Test shorthand: String -> Model
    let json_input = r####""example""####;
    let instance = Model::from_json(json_input).expect("Failed to load Model from String");
    assert_eq!(instance.id, "example");
}
